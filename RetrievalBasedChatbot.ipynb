{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "file = open('intents.json').read()\n",
    "file = json.loads(file)#loading the json file which has our intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "words=[]\n",
    "document=[]\n",
    "classes = []\n",
    "for intent in file['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        w = nltk.word_tokenize(pattern)# Change pattern into words 2D array\n",
    "        words.extend(w)#convert word into one array\n",
    "        document.append((w,intent['tag']))#append tag with sentence like (['Hi', 'there'], 'greeting')\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])#saving all the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()#WordNetLemmatizer used convert any word into actual form like believes into belief\n",
    "\n",
    "ignore_word = ['?','!']\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_word]#removing ignore word and converting into lower and origninal form of word\n",
    "\n",
    "words = sorted(list(set(words)))#sorting words and removing duplicates\n",
    "classes = sorted(list(classes))\n",
    "\n",
    "pickle.dump(words,open('words.pickle','wb'))#wb means writing in binary mode\n",
    "pickle.dump(classes,open('classes.pkl','wb'))\n",
    "#When we load use pickle.load(open('words.pickle','rb')) reading in binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 88)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Training set\n",
    "import random\n",
    "training = []\n",
    "output_empty = [0] * len(classes)#making a empty list with size of classes\n",
    "\n",
    "for doc in document:\n",
    "    pattern_word = doc[0]#taking pattern form the documents\n",
    "    bag=[]\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_word]#converting pattern into actual form\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)#what bag do make value one whenever word found in words array\n",
    "        #for making the training features\n",
    "    output_row = list(output_empty)\n",
    "    #print(classes.index(doc[1]))\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    #making labels\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "np.array(train_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "47/47 [==============================] - 0s 6ms/sample - loss: 2.2313 - acc: 0.1064\n",
      "Epoch 2/200\n",
      "47/47 [==============================] - 0s 161us/sample - loss: 2.2220 - acc: 0.0851\n",
      "Epoch 3/200\n",
      "47/47 [==============================] - 0s 189us/sample - loss: 2.1347 - acc: 0.2340\n",
      "Epoch 4/200\n",
      "47/47 [==============================] - 0s 194us/sample - loss: 2.0368 - acc: 0.2766\n",
      "Epoch 5/200\n",
      "47/47 [==============================] - 0s 174us/sample - loss: 2.0617 - acc: 0.2766\n",
      "Epoch 6/200\n",
      "47/47 [==============================] - 0s 202us/sample - loss: 1.8097 - acc: 0.4894\n",
      "Epoch 7/200\n",
      "47/47 [==============================] - 0s 200us/sample - loss: 1.7713 - acc: 0.5319\n",
      "Epoch 8/200\n",
      "47/47 [==============================] - 0s 212us/sample - loss: 1.5365 - acc: 0.6383\n",
      "Epoch 9/200\n",
      "47/47 [==============================] - 0s 186us/sample - loss: 1.3953 - acc: 0.6383\n",
      "Epoch 10/200\n",
      "47/47 [==============================] - 0s 231us/sample - loss: 1.1575 - acc: 0.5745\n",
      "Epoch 11/200\n",
      "47/47 [==============================] - 0s 169us/sample - loss: 1.2336 - acc: 0.7021\n",
      "Epoch 12/200\n",
      "47/47 [==============================] - 0s 172us/sample - loss: 1.0442 - acc: 0.6596\n",
      "Epoch 13/200\n",
      "47/47 [==============================] - 0s 178us/sample - loss: 0.9453 - acc: 0.6809\n",
      "Epoch 14/200\n",
      "47/47 [==============================] - 0s 167us/sample - loss: 0.8056 - acc: 0.7234\n",
      "Epoch 15/200\n",
      "47/47 [==============================] - 0s 195us/sample - loss: 0.9058 - acc: 0.6809\n",
      "Epoch 16/200\n",
      "47/47 [==============================] - 0s 155us/sample - loss: 0.7253 - acc: 0.7021\n",
      "Epoch 17/200\n",
      "47/47 [==============================] - 0s 194us/sample - loss: 0.8436 - acc: 0.6596\n",
      "Epoch 18/200\n",
      "47/47 [==============================] - 0s 348us/sample - loss: 0.4943 - acc: 0.8723\n",
      "Epoch 19/200\n",
      "47/47 [==============================] - 0s 384us/sample - loss: 0.3506 - acc: 0.9149\n",
      "Epoch 20/200\n",
      "47/47 [==============================] - 0s 182us/sample - loss: 0.6114 - acc: 0.8723\n",
      "Epoch 21/200\n",
      "47/47 [==============================] - 0s 135us/sample - loss: 0.3021 - acc: 0.9149\n",
      "Epoch 22/200\n",
      "47/47 [==============================] - 0s 290us/sample - loss: 0.3837 - acc: 0.8936\n",
      "Epoch 23/200\n",
      "47/47 [==============================] - 0s 230us/sample - loss: 0.3015 - acc: 0.9362\n",
      "Epoch 24/200\n",
      "47/47 [==============================] - 0s 204us/sample - loss: 0.3321 - acc: 0.8936\n",
      "Epoch 25/200\n",
      "47/47 [==============================] - 0s 140us/sample - loss: 0.1825 - acc: 0.9362\n",
      "Epoch 26/200\n",
      "47/47 [==============================] - 0s 192us/sample - loss: 0.2937 - acc: 0.8936\n",
      "Epoch 27/200\n",
      "47/47 [==============================] - 0s 217us/sample - loss: 0.1215 - acc: 0.9574\n",
      "Epoch 28/200\n",
      "47/47 [==============================] - 0s 156us/sample - loss: 0.4606 - acc: 0.8511\n",
      "Epoch 29/200\n",
      "47/47 [==============================] - 0s 195us/sample - loss: 0.2596 - acc: 0.9362\n",
      "Epoch 30/200\n",
      "47/47 [==============================] - 0s 201us/sample - loss: 0.2243 - acc: 0.9574\n",
      "Epoch 31/200\n",
      "47/47 [==============================] - 0s 158us/sample - loss: 0.2049 - acc: 0.8936\n",
      "Epoch 32/200\n",
      "47/47 [==============================] - 0s 152us/sample - loss: 0.2031 - acc: 0.9149\n",
      "Epoch 33/200\n",
      "47/47 [==============================] - 0s 172us/sample - loss: 0.1816 - acc: 0.9362\n",
      "Epoch 34/200\n",
      "32/47 [===================>..........] - ETA: 0s - loss: 0.0902 - acc: 1.0000\n",
      " accuracy is more than 99.9% so cancelling training\n",
      "47/47 [==============================] - 0s 155us/sample - loss: 0.0810 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#Making a model\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "#making callback for early stopping\n",
    "class mycallback(tensorflow.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if (logs.get('acc')>0.999):\n",
    "            print('\\n accuracy is more than 99.9% so cancelling training')\n",
    "            self.model.stop_training=True\n",
    "            \n",
    "model = Sequential([\n",
    "    Dense(128,input_shape=[len(train_x[0]),],activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(train_y[0]),activation='softmax')\n",
    "])\n",
    "#we want some kind of ‘moving’ average which would ‘denoise’ the data and bring it closer to the original function. thats why we used momentum\n",
    "model.compile(loss='categorical_crossentropy',optimizer=SGD(lr=0.1, momentum=0.9),metrics=['acc'])#I used momentum = 0.9 above. It is a good value and most often used in SGD with momentum\n",
    "model.fit(np.array(train_x),np.array(train_y),epochs=200,callbacks=[mycallback()])\n",
    "model.save_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(text,model):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in text]\n",
    "    bag=[]\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    y_pred = model.predict(np.array([bag]))\n",
    "    y = np.argmax(y_pred)\n",
    "    tag = classes[y]\n",
    "    print(tag)\n",
    "    intent = file['intents']\n",
    "    for i in intent:\n",
    "        if(i['tag'] == tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = model = Sequential([\n",
    "    Dense(128,input_shape=[len(train_x[0]),],activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(train_y[0]),activation='softmax')\n",
    "])\n",
    "\n",
    "new_model.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greeting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hi there, how can I help?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response('HEllO,How are you?',new_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
